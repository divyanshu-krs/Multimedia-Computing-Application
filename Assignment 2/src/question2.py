# -*- coding: utf-8 -*-
"""MCA_A2_2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11BNzCWNaDii-U0N0zPvPG0Kqv8TrfgkY
"""

from google.colab import drive
drive.mount('/content/drive')

import os
import numpy as np
from scipy.io import wavfile
import scipy.fftpack as fft
from scipy.fftpack import dct
from scipy.signal import get_window
import matplotlib.pyplot as plt

#references -> https://medium.com/@jonathan_hui/speech-recognition-feature-extraction-mfcc-plp-5455f5a69dd9
#references -> https://haythamfayek.com/2016/04/21/speech-processing-for-machine-learning.html

# Read the wav file
sample_rate, audio = wavfile.read("/content/drive/My Drive/MCA/Assignment 2/Dataset/training/41777abb_nohash_0.wav")

# Apply Pre-emphasis
## This is done to boost the higher frequence # alhpa is chosen between 0.95-0.99
def PreEmp(data):
  pre_emp_Factor = 0.96 * audio[:-1]
  boosted_signal = np.append(audio[0], audio[1:] - pre_emp_Factor)
  return boosted_signal

### Framing ###
def Framing_Pre_EmpSignal(audio,sample_rate,FFT_size=2048, frame_stride=10 ):

    audio = np.pad(audio, int(FFT_size / 2), mode='reflect')
    frame_step = frame_stride * sample_rate
    signal_length = len(audio)
#    frame_len = np.round(frame_step / 1000).astype(int)
#    frame_num = int((signal_length - FFT_size) / frame_len) + 1
#    frames = np.zeros((frame_num,FFT_size)
    
    return frames

def framing(audio, FFT_size, stride, sample_rate):
    audio = np.pad(audio, int(FFT_size / 2), mode='reflect')
    jump = sample_rate * stride
    frame_length = np.round(jump/ 1000).astype(int)
    num_fr = int((len(audio) - FFT_size) / frame_length) + 1
    frames = np.zeros((num_fr,FFT_size))
    
    for i in range(num_fr):
        frames[i] = audio[i*frame_length:i*frame_length+FFT_size]
    return frames

##### Choose the window
def choose_win(FFT_size):
  window = get_window("hann", FFT_size, fftbins=True)
  return window

#### Windowing the signal #####
def window_signal(framed_signal,FFT_size):
  audio_win = framed_signal * choose_win(FFT_size)
  return (audio_win)

def fft_signal(audio_win,FFT_size):
  audio_winT = np.transpose(audio_win)
  audio_fft = np.empty((int(1 + FFT_size // 2), audio_winT.shape[1]), dtype=np.complex64, order='F')
  for n in range(audio_fft.shape[1]):
    audio_fft[:, n] = fft.fft(audio_winT[:, n], axis=0)[:audio_fft.shape[0]]
    
  FFT_signal = np.transpose(audio_fft)
  return FFT_signal

def fft_power(FFT_signal):
  audio_power = np.square(np.abs(FFT_signal))
  return audio_power

def freq_to_mel(freq):
    return 2595.0 * np.log10(1.0 + freq / 700.0)

def met_to_freq(mels):
    return 700.0 * (10.0**(mels / 2595.0) - 1.0)

def get_filter(fmin, fmax, mel_filter_num, FFT_size, sample_rate):
    fmin_mel = freq_to_mel(fmin)
    fmax_mel = freq_to_mel(fmax)
    mels = np.linspace(fmin_mel, fmax_mel, num=mel_filter_num+2)
    freqs = met_to_freq(mels)
    filter_points = np.floor((FFT_size + 1) / sample_rate * freqs).astype(int)
    mel_freqs= freqs
    #filter_points, mel_freqs = get_filter_points(freq_min, freq_high, mel_filter_num, FFT_size, sample_rate=44100)
    filters = np.zeros((len(filter_points)-2,int(FFT_size/2+1)))
    for n in range(len(filter_points)-2):
        filters[n, filter_points[n] : filter_points[n + 1]] = np.linspace(0, 1, filter_points[n + 1] - filter_points[n])
        filters[n, filter_points[n + 1] : filter_points[n + 2]] = np.linspace(1, 0, filter_points[n + 2] - filter_points[n + 1])

    return (filters, mel_freqs)

def get_mfcc(audio,sample_rate,stride,FFT_size,freq_min,freq_high,mel_filter_num):
  framed_signal = framing(audio, FFT_size, stride, sample_rate)
  
  win = choose_win(FFT_size)
  audio_win = window_signal(framed_signal,FFT_size)
  
  FFT_signal = fft_signal(audio_win,FFT_size)
  audio_power = fft_power(FFT_signal)
  
  filters, mel_freqs = get_filter(freq_min, freq_high, mel_filter_num, FFT_size, sample_rate)
  enorm = 2.0 / (mel_freqs[2:mel_filter_num+2] - mel_freqs[:mel_filter_num])
  filters *= enorm[:, np.newaxis]
  audio_filtered = np.dot(filters, np.transpose(audio_power))
  audio_log = 10.0 * np.log10(audio_filtered)
  dct_filters = dct(filters, type=3, norm='ortho')[:, 1 : (40 + 1)]
  cepstral_coefficents = np.dot(dct_filters.T, audio_log)

  return cepstral_coefficents

## use the code below for individual data file else comment it
freq_min = 0
freq_high = sample_rate / 2
mel_filter_num = 10
stride = 15 
FFT_size = 2048
sampling_frequency = sample_rate
mfcc_try = get_mfcc(audio,sample_rate,stride,FFT_size,freq_min,freq_high,mel_filter_num)
mfcc_try[:,0]

### use the code below for making mfcc from large datasets like folders
import glob
import pandas as pd

sub_dirs = os.listdir('/content/drive/My Drive/MCA/Assignment 2/Dataset/train_v2')
sub_dirs.sort()
training_list = []
for label, sub_dir in enumerate(sub_dirs):  
  for file_name in glob.glob(os.path.join('/content/drive/My Drive/MCA/Assignment 2/Dataset/train_v2',sub_dir,"*.wav")):
      #print(file_name)
      sample_rate, audio = wavfile.read(file_name)
      freq_min = 0
      freq_high = sample_rate / 2
      mel_filter_num = 10
      stride = 15 
      FFT_size = 2048
      mf = get_mfcc(audio,sample_rate,stride,FFT_size,freq_min,freq_high,mel_filter_num)
      training_list.append([mf[:,0],label])



sub_dirs = os.listdir('/content/drive/My Drive/MCA/Assignment 2/Dataset/train_v5')
sub_dirs.sort()

for label, sub_dir in enumerate(sub_dirs):  
  for file_name in glob.glob(os.path.join('/content/drive/My Drive/MCA/Assignment 2/Dataset/train_v5',sub_dir,"*.wav")):
      #print(file_name)
      sample_rate, audio = wavfile.read(file_name)
      freq_min = 0
      freq_high = sample_rate / 2
      mel_filter_num = 10
      stride = 15 
      FFT_size = 2048
      mf = get_mfcc(audio,sample_rate,stride,FFT_size,freq_min,freq_high,mel_filter_num)
      training_list.append([mf[:,0],label])

len(training_list)
training_list = np.nan_to_num(training_list)

training_df = pd.DataFrame(training_list,columns = ['feature','class_label'])

sub_dirs = os.listdir('/content/drive/My Drive/MCA/Assignment 2/Dataset/validation')
sub_dirs.sort()
testing_list = []
for label, sub_dir in enumerate(sub_dirs):  
  for file_name in glob.glob(os.path.join('/content/drive/My Drive/MCA/Assignment 2/Dataset/validation',sub_dir,"*.wav")):
      #print(file_name)
      sample_rate, audio = wavfile.read(file_name)
      freq_min = 0
      freq_high = sample_rate / 2
      mel_filter_num = 10
      stride = 15 
      FFT_size = 2048
      mf = get_mfcc(audio,sample_rate,stride,FFT_size,freq_min,freq_high,mel_filter_num)
      testing_list.append([mf[:,0],label])

testing_df = pd.DataFrame(testing_list,columns = ['feature','class_label'])
testing_df.head

training_df.to_csv('/content/drive/My Drive/MCA/Assignment 2/Dataset/MFCC_TRAIN.csv')
testing_df.to_csv('/content/drive/My Drive/MCA/Assignment 2/Dataset/MFCC_TEST.csv')













